---
title: "크롤러 개발과 문제점 해결에 관한 글(2)"
date: 2020-11-16
categories: tech_journal
tags: tech_journal
author_profile: true
toc: true
toc_sticky: true
---

## 1. Background

[지난글](https://orezmis.github.io/tech_journal/journal_01/)

코드를 제출하고 난뒤 몇가지 개선점을 피드백 받았다.

* 이미지 파일의 크롤링 속도 개선
* ```range()``` 함수가 아닌 외부 csv 파일에서 프로젝트 ID를 읽어와 크롤링을 하도록 할것

크롤러의 속도개선은 피드백을 받기 전에도 구상을 하던 것이였고, 확실히 ```range()``` 함수로 프로젝트 id를 읽는방법은 범용적이지도 못하거니와 부분적으로 크롤링하고 싶은 id를 선택할 수 없기에 문제가 있다.

제공받은 예시 csv의 양식은 아래와 같은 구조로 되어 있었다.

|project|
|-------|
|322|
|323|
|521|
|1032|
|....|

하나의 컬럼과 엔터 delimiter로 구성된 매우 단순한 구조의 csv다.
이번엔 총 36개의 프로젝트, 약 68000여개의 JSON과 이미지 파일을 크롤링해야 하기 때문에 코드의 개선이 필요하다.




## 2. Objective

위의 피드백과 더불어 크롤링할 파일이 많아진 만큼 몇가지 개선된 기능을 추가해보려 했다.

* 크롤링이 도중에 중단되어도 이어받을 수 있게끔 기능 개선
* 크롤링 완료 후 결과창 추가




## 3. Codes


```python
import csv
import requests
import json
import os
import random
import time




# CVS 에서 파일 읽어오기
# read projectId from CSV
with open('project_id_collection.csv') as filestream:
    idList = []
    for line in csv.reader(filestream, delimiter=';'):
        try:
            idList.append(int(line[0].replace(',', '')))
        except ValueError:
            continue




# json 로딩
inpt = idList # GLOBAL VARIABLE

def slidingURL(inpt):
    r = pd.Series([])
    for i in inpt:
        deltaURL = 'http://127.0.0.1:9000/images?id=' + str(i)
        res = json.loads(requests.get(deltaURL).text)
        r[i] = res
        
    return r

rawJson = slidingURL(inpt).reset_index(drop=True)
```
``` r = pd.Series([]) ``` 부분은 ``` r = [] ``` 으로, 
``` r[i] = res ```를 ``` r.append[deltaURL] ``` 로 하여도 동일한 기능을 한다
다만 ipython 환경에서 작업할때 결과창을 조금 더 읽기 쉽게 만들어 준것 뿐이다.



```python
# json 파일 폴더 생성
dir = "./test_set/batch_test/" # directory path subject to change

def mkDir(dir, inpt):
    des = []
    try:
        for i in inpt:
            ndir = dir + str(i) + "/" + str(i) + "_json"
            if not os.path.exists(ndir):
                os.makedirs(ndir)
                des.append(ndir)
                print("directory " + str(ndir) + " created")
            elif os.path.exists(ndir):
                des.append(ndir)
                print("directory " + str(ndir) + " already exist")
    except OSError:
            print("Error while processing command")
    return des

destination = mkDir(dir, inpt)




# json 파일 크롤링
def jsonCrawl(ndir):
    idxcnt = 0
    for i in range(0, len(rawJson)):
        try:
            loc = ndir[i]
            t = rawJson[i]
            fileDetection = loc + "/" + t[random.randrange(len(t))].get("originalName") + ".json"

            for d in range(idxcnt, idxcnt+1):
                if not os.path.isfile(fileDetection):

                    print("")
                    print("==========================================")
                    print("Initiating json download with projectId " + str(inpt[i]))
                    #for j in range(idxcnt, idxcnt+1):
                    for j in range(0, len(t)):
                        resource = json.dumps(t[j], indent=4)
                        p = open(loc + "/" + \
                                str(t[j].get("originalName")) + \
                                ".json", "a")
                        p.write(resource)
                        p.close()
                    idxcnt += 1

                    print("")
                    print("projectId " + str(inpt[i]) + " download completed")
                    print("==========================================")
                    print("")

                elif os.path.isfile(fileDetection):
                    print("files in directory " + loc + " already exist!")
                    idxcnt += 1

        except OSError:
             print("Error while processing command")
            
    print("Download Completed")
jsonCrawl(destination)
```
for 문이 무려 3개나 중첩되었고 나중에 코드를 다시 뜯을때 고생하게 될거라는걸 이땐 몰랐다.
루프문은 2개 이상 중첩되면 코드를 읽기 난해해지며 (이땐 완벽한 OOP로 작성하는줄 알았지만) OOP 스타일의 코딩과는 거리가 있다.

``` idxcnt ``` 라는 변수를 설정하여 맨 안쪽 루프문이 끝날때마다 카운트수가 올라게게끔 설정했고
전체  ``` len(rawJson) ``` 만큼 순회하되 ``` idxcnt ``` 부터 ``` idxcnt + 1 ``` 까지, 한개씩 순차적으로 ``` rawJson ```의 인덱스를 크롤링 할것인지 패스할것인지 구현했다.
이 기능으로 성공적으로 크롤링 도중 멈추게 되면 다시 멈춘 지점부터 이어받을 수 있게 되었다.



```python
# 이미지 파일 폴더 생성
imgdir = "./test_set/batch_test/" # directory path subject to change upon request

def mkImgDir(imgdir, inpt):
    des = []
    try:
        for i in inpt:
            ndir = dir + str(i) + "/" + str(i) + "_img"
            if not os.path.exists(ndir):
                os.makedirs(ndir)
                des.append(ndir)
                print("directory " + str(ndir) + " created")
            elif os.path.exists(ndir):
                des.append(ndir)
                print("directory " + str(ndir) + " already exist")
    except OSError:
            print("Error while processing command")
    return des

imgDest = mkImgDir(imgdir, inpt)




# 이미지 파일 크롤링
def imgCrawl(ndir):
    idxcnt = 0
    for d in range(0, len(ndir)):
        try:
            loc = ndir[d]
            t = rawJson[d]
            imgDetection = loc + "/" +  \
                           t[random.randrange(len(t))].get("originalName") \
                           + ".jpg"
            if not os.path.isfile(imgDetection):
                
                #--Codes with hashtag for debugging purpose--
                #print()
                #print("=========")
                #print(loc)
                #print("idxcnt: " ,idxcnt)
                
                print("")
                print("Initiating images download with projectId " + str(inpt[d]))
                for i in range(idxcnt, idxcnt+1):
                    for j in range(0, len(rawJson[i])):
                    #for j in range(0, 20):
                        deltaimgURL = 'http://127.0.0.1:9000/images/' + \
                                    str(inpt[i]) + '/' + \
                                    str(rawJson[i][j]['id'])
                        img = requests.get(deltaimgURL)
                        #print(deltaimgURL)
                        f = open(loc + "/" + \
                             str(rawJson[i][j]['originalName']) + \
                             ".jpg", 'wb')
                        f.write(img.content)
                        f.close()
                        time.sleep(0.05)
                    idxcnt += 1
                print("")
                print("projectId " + str(inpt[d]) + " download completed")
                print("==========================================")
        
            elif os.path.isfile(imgDetection):
                    print("files seem to be present already. Check " + loc + " directory")
                    idxcnt += 1 
                    
        except OSError:
            print("Error while downloading images")

    #print()
    #print("=========")
    #print()
    #print("final idxcnt: " ,idxcnt)

    print("Download Completed")
imgCrawl(imgDest)
```


기능적으론 위의 JSON 파일을 크롤링 하는것과 동일하나, 역시 많은 루프문을 사용하여 나름 오토매틱하게 이름을 설정하고 경로를 찾아가게끔 만들었다.
``` time.sleep(0.05) ```의 시간차를 줘 더 빠르게 이미지를 저장하게끔 했고 문제는 없는듯 했다.
이때까지만 해도 머리를 써가며 많은 중첩 루프문을 사용하여 코드를 짠게 대견하다고 생각 했지만 다음에 마주할 문제때문에 생 헛고생이였다는걸 알게 되었다.




## 4. Problems

* 기존 크롤러보다 성능은 좋아졌지만 이미지를 하나하나 저장하고 다음 프로젝트 이미지를 저장하는 직렬화 코드이기에 아직도 느리다.
* 여러개 프로젝트 이미지가 동시에 저장되는 병렬화 코드로 전환을 해야할 필요가 있다.
* 랜덤한 파일을 찾고 그것을 기준삼아 파일이 다 받아졌는지 아닌지 확인하는 중복처리는 아직 개선이 필요하다.
* 이 중복처리를 사용하여 이어받기를 판단하기 때문에 아직 이어받기 기능은 완벽하지 못하다.
* 전체 다운받는 시간이 얼마나 걸리는지 확인할 친절한 결과창이 필요하다.



총 36개의 프로젝트 폴더로부터 약 68000개의 JSIN 파일과 이미지 파일들을 다운받았고, 전체적으로 약 4~5시간 가량이 걸린듯 했다.
아직 개선의 부분이 남아있고 다음 글에서 어떻게 위의 문제들을 해결하였는지 적어보겠다,,,